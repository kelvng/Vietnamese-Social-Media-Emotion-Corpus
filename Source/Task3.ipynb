{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Task3 LSTM + Attention.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ndnMfSlzIVg"
   },
   "source": [
    "#**LSTM + Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeNTCKnKnN6R"
   },
   "source": [
    "# I.Import Library\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AN8rgbmrdKJP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5e5261d4-c731-4e63-8521-0f458270b541"
   },
   "source": [
    "\n",
    "%tensorflow_version 1.x\n",
    "!pip install keras==2.2.5\n",
    "!pip install pyvi\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import os, pickle, re, keras, sklearn, string\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import gensim, operator, json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import *\n",
    "import keras.backend as K\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers\n",
    "from keras import optimizers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import constraints"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n",
      "Requirement already satisfied: keras==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from keras==2.2.5) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n",
      "Requirement already satisfied: pyvi in /usr/local/lib/python3.7/dist-packages (0.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.22.2.post1)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
      "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.41.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqqjA5rmnXW9"
   },
   "source": [
    "# II.Read Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f_QyBjmaFumz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9d7df652-095a-4a33-9265-2de3f7f91c16"
   },
   "source": [
    "!wget https://thiaisotajppub.s3-ap-northeast-1.amazonaws.com/publicfiles/baomoi.model.bin"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "--2021-05-09 10:38:09--  https://thiaisotajppub.s3-ap-northeast-1.amazonaws.com/publicfiles/baomoi.model.bin\n",
      "Resolving thiaisotajppub.s3-ap-northeast-1.amazonaws.com (thiaisotajppub.s3-ap-northeast-1.amazonaws.com)... 52.219.4.63\n",
      "Connecting to thiaisotajppub.s3-ap-northeast-1.amazonaws.com (thiaisotajppub.s3-ap-northeast-1.amazonaws.com)|52.219.4.63|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 708212586 (675M) [application/macbinary]\n",
      "Saving to: ‘baomoi.model.bin.2’\n",
      "\n",
      "baomoi.model.bin.2  100%[===================>] 675.40M  20.1MB/s    in 36s     \n",
      "\n",
      "2021-05-09 10:38:46 (18.7 MB/s) - ‘baomoi.model.bin.2’ saved [708212586/708212586]\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ww3kuJBNFyT_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "78f8d1f0-e58f-473d-8dea-3ed31782be6e"
   },
   "source": [
    "!wget https://github.com/nthanhkhang/Vietnamese-Social-Media-Emotion-Corpus/raw/main/Data/Data.zip"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "--2021-05-09 10:38:46--  https://github.com/nthanhkhang/Vietnamese-Social-Media-Emotion-Corpus/raw/main/Data/Data.zip\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/nthanhkhang/Vietnamese-Social-Media-Emotion-Corpus/main/Data/Data.zip [following]\n",
      "--2021-05-09 10:38:46--  https://raw.githubusercontent.com/nthanhkhang/Vietnamese-Social-Media-Emotion-Corpus/main/Data/Data.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 25932 (25K) [application/zip]\n",
      "Saving to: ‘Data.zip.1’\n",
      "\n",
      "Data.zip.1          100%[===================>]  25.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-05-09 10:38:46 (100 MB/s) - ‘Data.zip.1’ saved [25932/25932]\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L6I-G2B7CwyA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2e5e1f00-ba36-4cdd-bf2e-cb96d6e2fc6a"
   },
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"Data.zip\",\"r\") as zf:\n",
    "    zf.extractall('Data')\n",
    "print(zf)"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "<zipfile.ZipFile [closed]>\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rp1-Qa_81FEY"
   },
   "source": [
    "path_train ='Data/link1.csv'\n",
    "path_valid ='Data/link2.csv'\n",
    "path_test ='Data/link3.csv'\n",
    "path_stopword = 'Data/stopwords.txt'"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3JUFIaAndQo"
   },
   "source": [
    "# III.Word2vec using baomoi.model.bin\n",
    "\n",
    "*   Function reading pretrain word embedding library.\n",
    "*   The word embedding pretrain has been trained in new news, 300-way news\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Skg7lUZ9LYAn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "256ac0ae-862d-4442-aeb4-dd1de70b48f6"
   },
   "source": [
    "path_embedding= 'baomoi.model.bin'\n",
    "\n",
    "import io\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_embedding = KeyedVectors.load_word2vec_format(path_embedding, binary=True)\n",
    "# Example of taking vector of 1 word in the word embedding pretrain\n",
    "EMBEDDING_DIM = word_embedding['yêu'].shape[0]\n",
    "print(\"Embedding: \",EMBEDDING_DIM)\n",
    "# Vector of love words in pretrained word embedding set.\n",
    "print(word_embedding['yêu'])\n"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Embedding:  400\n",
      "[-0.78774583 -0.22327825 -0.6274532  -2.7228408  -2.2186291   0.38002455\n",
      "  3.8660462   0.9853684  -1.4683082  -1.7013292  -0.5839958  -0.14467287\n",
      "  3.600142    3.381808   -0.02930526  3.0047843  -0.2006207  -1.0937127\n",
      "  1.7360235   2.3691583  -0.71597415  3.319453    0.2824182  -3.0814204\n",
      "  2.6810844  -0.810977    1.5186927  -2.10329     1.3271075  -1.3646411\n",
      " -0.11144319 -4.6505136  -1.7251624  -2.31126     1.583203   -0.8746506\n",
      " -2.6937015  -1.7733976   0.557898   -1.7562917   1.3282276  -0.3805479\n",
      " -1.3979301  -0.1536707  -1.1909302   1.3283668   0.22275637 -2.7959821\n",
      " -5.188217   -0.6404673   0.0164395   0.67177856 -1.4948794   0.21867418\n",
      " -1.4103376   0.99262404  2.2180524  -0.4881204   3.0988753  -0.31382522\n",
      "  1.3226501   0.21269594 -1.6409203   1.7758838   2.3379912  -2.4666297\n",
      " -0.599687    0.551105   -1.3755493  -1.4293027  -2.6366289   0.40759587\n",
      " -0.77850854 -0.6169452  -0.84525913  0.02801617  2.1296268   0.13715844\n",
      " -1.1562283  -2.1226277  -0.1346792   0.88932824 -1.5711976   0.36148685\n",
      "  2.2572796  -2.0762215   1.736077   -0.3133224   0.48849696  2.1262195\n",
      " -2.3417432   0.7264937   1.3197432  -1.0578146   1.9603167  -1.957219\n",
      " -0.47556064 -3.2944543  -1.540249    1.6060241   0.02990843 -1.0645736\n",
      " -0.5550473   1.589397   -0.5811684  -1.2199221  -0.9025384  -1.2436662\n",
      "  0.50163126  0.11698119  0.8760743   0.8978141  -1.8893797  -0.1424527\n",
      " -3.0423136  -0.88489795  0.49000955 -3.4689097  -1.8564429  -0.66697997\n",
      "  5.3912683  -2.092744   -0.5973023  -0.7118058  -1.0953093   0.4417508\n",
      "  2.440871    1.1271865   1.4602836  -2.714987   -1.3927895  -0.16143057\n",
      "  0.07596377 -2.0885456  -1.0929846  -1.1670731   0.7352281  -1.0726835\n",
      "  0.4963534  -0.78458273  2.3078787  -3.5055773   0.9567256  -0.5207236\n",
      "  0.36697528  0.8511779   0.878965    1.3028007   0.04724613  0.9892602\n",
      " -0.8373782   0.27926713  2.268885   -0.11917569  0.8163163   0.3869213\n",
      "  0.20561185 -2.2969527  -1.6468542  -3.9922614  -0.96281457  3.2537632\n",
      "  0.48358652 -0.6078726   3.2632709   0.11489751 -2.6600893   0.92677915\n",
      " -0.528953    3.4760187   2.31958     2.23189     2.2253554  -1.8307585\n",
      " -1.7324418  -1.2364737  -4.273679    0.9341229   0.59669524 -0.03376843\n",
      " -2.971719    1.9712305  -0.549242    0.4829846   1.4618144  -1.3703161\n",
      " -1.1212839   0.4291749   1.4675773  -0.67144257 -0.49444234  1.7652586\n",
      "  1.7143794   0.54265493  2.1978571   3.2426474  -2.7528286  -2.2640996\n",
      "  0.09805597  1.2702079   1.156494   -1.1671641  -2.3361897   1.2424865\n",
      " -2.1413488  -0.22989413 -2.7570245  -1.2689328  -0.11422111  0.340871\n",
      " -0.72356385 -3.100242   -0.2113436   0.08352826  3.0843058   0.2549431\n",
      "  3.6576512  -0.71284246 -1.8232595  -1.0566906  -0.7372802   0.18872899\n",
      "  0.11927979  3.0378866   0.7687284   0.7458194   3.392024   -0.10601766\n",
      "  1.7550762  -3.9328496   0.5543825   2.4240685   2.4877627  -1.8583341\n",
      " -2.7361338   0.9327119   1.4136555   0.6736002  -0.56006515 -0.17299697\n",
      "  2.3964696   1.4890865   0.47563386 -1.7579868  -3.2750478  -2.711356\n",
      "  1.1631078   0.5226146  -0.77252626  2.0378802  -2.1662908   1.1695647\n",
      "  1.0302314   1.2815226   1.8774925   1.0482382  -2.829525   -1.3818443\n",
      "  1.0274167  -0.61302423  0.24060939 -2.8208141   1.2254591  -1.9544963\n",
      "  1.7342434  -0.9264713   0.39906958  1.4076114   0.68284744  2.4939642\n",
      "  2.1315014   0.20849855 -1.4851028   4.6376705   2.2327776   2.2943447\n",
      " -1.253777   -2.385657   -2.5678577  -1.7067193   2.387362   -1.3572613\n",
      " -4.5016227   0.7827232  -0.44352373  0.3998332  -0.5178318   0.6794015\n",
      "  3.7974224  -0.774167   -1.1981938   0.3985697  -1.8760519   0.1238703\n",
      "  0.05213618 -1.1321199  -2.8599005  -1.7278007   2.1998515  -2.5468414\n",
      "  0.9428754   0.992005   -2.7554674  -1.364683   -0.8704408  -1.1697435\n",
      "  1.880865    1.9564949   3.1174672   0.14133504 -2.2360458  -1.173718\n",
      "  1.3261789  -2.2110426   0.589773    3.4267988   3.2046275  -0.91721445\n",
      "  0.7951813  -2.3174386  -0.8497346   1.6120318  -0.40876418  0.7933062\n",
      " -0.8393237  -0.41007656  2.8945262   0.993892   -1.7588191  -4.925732\n",
      " -2.4419074   3.0766954  -1.9000514  -2.0604458   3.4133394  -2.0102491\n",
      "  2.046963    0.71078193  2.2965722   1.72548     1.2269657  -2.2357993\n",
      "  0.50008225  4.8475957  -1.0541344   2.308317   -1.1630418   1.1258802\n",
      " -1.2478187   1.0942221   0.92810786 -1.5838045  -2.0654778  -0.28107494\n",
      "  2.4073355   1.3492072   0.48608387 -0.4716219   2.1616933  -1.3125483\n",
      "  1.5123384  -4.5521007  -1.8622614   1.088746    1.6043898  -4.609651\n",
      " -0.4500263   1.9802842   1.9102592  -0.47483805 -4.2590923   1.1523023\n",
      "  0.9832213   1.9767743   3.2593958   0.41252086  0.37052628  0.5532108\n",
      " -0.44213554  1.3767333  -0.8914752  -4.444391   -1.9290444  -1.6292545\n",
      " -2.6812217  -1.0284953   0.0313996   4.0027394   0.6476944  -0.02785059\n",
      " -2.4854484  -0.82289666  1.3539648   3.0980484  -1.3214976  -1.5370079\n",
      "  0.96148336  0.1992502   3.5529153   2.4300497 ]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo7k112MoVAQ"
   },
   "source": [
    "# IV. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5Gb-4LVo1bf"
   },
   "source": [
    "## 1.Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MWTniSb1o0d9"
   },
   "source": [
    "def tokenizer(text):\n",
    "    token = ViTokenizer.tokenize(text)\n",
    "    return token"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsAfz-Kco86j"
   },
   "source": [
    "## 2.Delete Icon"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VCchYWSMo9y5"
   },
   "source": [
    "def deleteIcon(text):\n",
    "    text = text.lower()\n",
    "    s = ''\n",
    "    pattern = r\"[a-zA-ZaăâbcdđeêghiklmnoôơpqrstuưvxyàằầbcdđèềghìklmnòồờpqrstùừvxỳáắấbcdđéếghíklmnóốớpqrstúứvxýảẳẩbcdđẻểghỉklmnỏổởpqrstủửvxỷạặậbcdđẹệghịklmnọộợpqrstụựvxỵãẵẫbcdđẽễghĩklmnõỗỡpqrstũữvxỹAĂÂBCDĐEÊGHIKLMNOÔƠPQRSTUƯVXYÀẰẦBCDĐÈỀGHÌKLMNÒỒỜPQRSTÙỪVXỲÁẮẤBCDĐÉẾGHÍKLMNÓỐỚPQRSTÚỨVXÝẠẶẬBCDĐẸỆGHỊKLMNỌỘỢPQRSTỤỰVXỴẢẲẨBCDĐẺỂGHỈKLMNỎỔỞPQRSTỦỬVXỶÃẴẪBCDĐẼỄGHĨKLMNÕỖỠPQRSTŨỮVXỸ,._]\"\n",
    "    for char in text:\n",
    "        if char !=' ':\n",
    "            if len(re.findall(pattern, char)) != 0:\n",
    "                s+=char\n",
    "            elif char == '_':\n",
    "                s+=char\n",
    "        else:\n",
    "            s+=char\n",
    "    s = re.sub('\\\\s+',' ',s)\n",
    "    return s.strip()"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5xJDr9epIEJ"
   },
   "source": [
    "## 3.Clean Doc"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L1_TFsm9WdA0"
   },
   "source": [
    "def clean_doc(doc):\n",
    "    doc = tokenizer(doc)\n",
    "    for punc in string.punctuation:# delete all punctuation (!,? ..) in a sentence\n",
    "        if punc != \"_\":\n",
    "            doc = doc.replace(punc,' ')\n",
    "    doc = deleteIcon(doc) \n",
    "    doc = re.sub(r\"[0-9]+\", \" num \", doc)# Delete numbers\n",
    "    doc = doc.lower()#lowercase \n",
    "    doc = re.sub('\\\\s+',' ',doc)# Remove lots of spaces\n",
    "    return doc"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJlgGwaxudQc"
   },
   "source": [
    "## 4.Stopword"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hPeiHFV2vRMY"
   },
   "source": [
    "# from underthesea import word_tokenize\n",
    "def pre_process(questions):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    questions_stop = [[t for t in tokens if (t not in stop_words) and (3 < len(t.strip()) < 15)]\n",
    "                      for tokens in questions_tokens]\n",
    "    questions_stop = pd.Series(questions_stop)\n",
    "    return questions_stop"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpylx70bxTGD"
   },
   "source": [
    "## 5.Word Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmCkYJpypwlS"
   },
   "source": [
    "# V.Train/Test data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Usy-4ygPWdA2"
   },
   "source": [
    "train_data = pd.read_csv(path_train,encoding='utf-16')\n",
    "valid_data = pd.read_csv(path_valid,encoding='utf-16')\n",
    "test_data = pd.read_csv(path_test,encoding='utf-16')\n",
    "\n",
    "X_train = train_data[\"Sentence\"].apply(lambda x : clean_doc(x))\n",
    "y_train = train_data[\"Emotion\"]\n",
    "\n",
    "X_val = valid_data[\"Sentence\"].apply(lambda x : clean_doc(x))\n",
    "y_val = valid_data[\"Emotion\"]\n",
    "\n",
    "X_test = test_data[\"Sentence\"].apply(lambda x : clean_doc(x))\n",
    "y_test = test_data[\"Emotion\"]"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GnYcGTbL0jnu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b946d8d1-12f9-46c7-daeb-731d4821affd"
   },
   "source": [
    "print(len(X_train),len(y_train))\n",
    "print(len(X_val),len(y_val))\n",
    "print(len(X_test),len(y_test))\n"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "100 100\n",
      "100 100\n",
      "100 100\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyPnd4f4qDSJ"
   },
   "source": [
    "## 1.Catalog vector"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fyIQeZi-WdA4"
   },
   "source": [
    "classes = ['Anger','Disgust','Enjoyment','Fear','Other','Sadness','Surprise']\n",
    "def to_category_vector(label):\n",
    "    vector = np.zeros(len(classes)).astype(np.float64)\n",
    "    index = classes.index(label)\n",
    "    vector[index] = 1.0\n",
    "    return vector"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ppniGtjqfhg"
   },
   "source": [
    "## 2.Convert labels to numbers in train and test practice"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fqYyTPmLWdA_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b1cda5e9-c9ec-4ef7-9c72-2d4be3c05916"
   },
   "source": [
    "y_train_encode = []\n",
    "for label in y_train:\n",
    "    y_train_encode.append(to_category_vector(label))\n",
    "\n",
    "\n",
    "y_val_encode = []\n",
    "for label in y_val:\n",
    "    y_val_encode.append(to_category_vector(label))\n",
    "\n",
    "print(classes)\n",
    "print(y_train_encode[0])\n",
    "print(y_train[0])"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "['Anger', 'Disgust', 'Enjoyment', 'Fear', 'Other', 'Sadness', 'Surprise']\n",
      "[0. 0. 1. 0. 0. 0. 0.]\n",
      "Enjoyment\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hK6nyuqOq171"
   },
   "source": [
    "## 3.LSTM\n",
    "\n",
    "\n",
    "*   All the words in the X_train set will form a dictionary\n",
    "*   Each vector of the input word, it will turn into a vector with a fixed number of dimensions and each vocabulary will be replaced by its index in the dictionary\n",
    "* Number of vector dimensions per input we will take the longest sentence which is the direction of the vector and the shorter arcs will automatically add the value 0 after"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0EB_NYPlWdBC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "90d1827a-365f-4d9d-d569-249a755b4215"
   },
   "source": [
    "xLengths = [len(x.split(' ')) for x in X_train]\n",
    "h = sorted(xLengths)  #sorted lengths\n",
    "maxLength =h[len(h)-1]\n",
    "print(\"The longest sentence length value: \",maxLength)\n",
    "input_tokenizer = Tokenizer(filters=\"\",oov_token=\"UNK\")\n",
    "input_tokenizer.fit_on_texts(X_train)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "word_index = input_tokenizer.word_index\n",
    "print(\"input_vocab_size:\",input_vocab_size)\n",
    "X_train_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_train), maxlen=maxLength,padding=\"post\"))"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "The longest sentence length value:  114\n",
      "input_vocab_size: 452\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rA3sXud4rkcS"
   },
   "source": [
    "## 4.Enter the example using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KxIekrnfrfBg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "032c465f-2ea7-4b81-8b95-3a6217b4dca7"
   },
   "source": [
    "print(\"Input String : \", X_train[0])\n",
    "print(\"Encode : \",X_train_encode[0])\n",
    "\n",
    "X_val_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_val), maxlen=maxLength,padding=\"post\"))"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Input String :  một giọng nói của người trải qua sự bi_đát mong được lên tv some day\n",
      "Encode :  [ 39  18  57  22  40 176  82 112 113  14  23   5  10 114 115   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzqNhkCQr7gr"
   },
   "source": [
    "## 5.Generate Embedding\n",
    "Function takes the vector of vocabulary in pre-trained word embedding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Td-ML6bqWdBE"
   },
   "source": [
    "def generate_embedding(word_index, model_embedding,EMBEDDING_DIM):\n",
    "    count6 = 0\n",
    "    countNot6 = 0\n",
    "    #embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM)) \n",
    "    embedding_matrix = np.asarray([np.random.uniform(-0.01,0.01,EMBEDDING_DIM) for _ in range((len(word_index) + 1))])\n",
    "    list_oov = []\n",
    "    word_is_trained = []\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model_embedding[word]\n",
    "            word_is_trained.append(word)\n",
    "        except:\n",
    "            continue\n",
    "        if embedding_vector is not None:\n",
    "            count6 +=1\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    print('Number of words in pre-train embedding: ' + str(count6))\n",
    "    print('Number of words not in pre-train embedding: ' + str(countNot6))\n",
    "    return embedding_matrix,word_is_trained"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HRIghLqMBuhV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "32d9765b-b733-4a27-ec78-f7ea6b94aa26"
   },
   "source": [
    "embedding_matrix,word_is_trained = generate_embedding(word_index,word_embedding,EMBEDDING_DIM)\n",
    "print(word_is_trained)"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Number of words in pre-train embedding: 424\n",
      "Number of words not in pre-train embedding: 0\n",
      "['anh', 'em', 'cho', 'lên', 'đi', 'ơi', 'là', 'rồi', 'tv', 'a', 'chú', 'có', 'mong', 'tuna', 'không', 'chị', 'giọng', 'tuân', 'e', 'thì', 'của', 'được', 'vợ', 'nhà', 'cháu', 'năm', 'ko', 'còn', 'để', 'với', 'khi', 'ạ', 'show', 'và', 'bị', 'đã', 'con', 'một', 'người', 'gì', 'thấy', 'cừu', 'hay', 'xem', 'nữa', 'mặt', 'tivi', 'lần', 'nghe', 'kể', 'sống', 'video', 'thứ', 'ra', 'nào', 'nói', 'đến', 'ông', 'như', 'giống', 'xong', 'phải', 'đấy', 'cái', 'nhưng', 'lắm', 'chuyện', 'nay', 'cay', 'vào', 'sofa', 'này', 'yêu', 'ngủ', 'ngoài', 'đc', 'vài', 'về', 'thật', 'sau', 'qua', 'nhất', 'có_thể', 'cũng', 'nghĩ', 'nên', 'mình', 'nhau', 'tuổi', 'đó', 'làm', 'mẹ', 'clip', 'someday', 'lâu', 'thôi', 'mà', 'ngày', 'năm_ngoái', 'live', 'nhớ', 'group', 'chờ', 'biết', 'vì', 'nha', 'r', 'k', 'quen', 'hết', 'sự', 'bi_đát', 'some', 'day', 'thích', 'cj', 'phết', 'lời', 'buồn', 'bây_giờ', 'vs', 'vui', 'anh_em', 'v', 'quý', 'trong', 'chứ', 'tiếp', 'chỉ', 'đợi', 'chu', 'cứ', 'thồn', 'sao', 'nhỉ', 'cô', 'từ', 'trên', 'stream', 'lại', 'vẫn', 'sống_mũi', 'mãi', 'liệu', 'tập', 'mất', 'não', 'đăng_ký', 'nói_xấu', 'rất', 'đẹp_trai', 'nghĩ_lại', 'mấy', 'câu', 'hỏi', 'đen', 'tính', 'vậy', 'thế_nào', 'mọi', 'vãi', 'thẩm_định', 'tưởng', 'lấy', 'hơn', 'nhiêu', 'vid', 'nhiều', 'tôi', 'câu_chuyện', 'ghế', 'giường', 'chơi', 'trải', 'chào', 'ăn', 'món', 'xinh', 'gửi', 'chúc', 'sức_khỏe', 'sóng', 'mãi_mãi', 'ủng_hộ', 'đọc', 'radio', 'a_vậy', 'hoàn_cảnh', 'đỡ', 'dell', 'muốn', 'kết_hôn', 'đầu', 'nhì', 'đít', 'mông', 'lòng', 'ra_điều', 'các', 'bạn', 'à', 'ôi', 'cày', 'view', 'hơi', 'oi', 'o', 'quan', 'phuong', 'nao', 'chu_cha', 'loi', 'hehe', 'nhẹ', 'lạy', 'cụ', 'tổ', 'ông_cha', 'bà', 'xin', 'rõ_ràng', 'ngồi', 'chồng', 'kề', 'xấu', 'cảm_giác', 'cơm', 'chó', 'ấy', 'anh_chị', 'tháng', 'sinh_nhật', 'ba', 'tự_hào', 'may', 'dựng', 'thành', 'đê', 'https', 'www', 'facebook', 'com', 'groups', 'đau_thương', 'nói_chuyện', 'cố', 'phim', 'n', 'like', 'mún', 'dung_nhan', 'đàn_bà', 'qué', 'á', 'triệu', 'liêm_sỉ', 'cảm', 'tội_lỗi', 'trở_thành', 'youtuber', 'mạng', 'tốt', 'đánh', 'bao_nhiêu', 'từng', 'đang', 'tạt', 'nước', 'hay_là', 'đường', 'nước_mắt', 'tuôn', 'trào', 'thương', 'vl', 'tích_cực', 'quảng_cáo', 'fb', 'tránh', 'giả', 'phát_triển', 'tận', 'luôn', 'ư', 'chung', 'tin', 'read', 'love', 'gợi_ý', 'ngoại_hình', 'cao', 'm', 'da', 'obama', 'gọi', 'đồng_hương', 'trẻ_con', 'thay_đổi', 'ny', 'cũ', 'thèm', 'gym', 'đứa', 'bn', 'trả_giá', 'đắt', 'lỡi', 'tay', 'tìm', 'tồn_tại', 'mk', 'đem', 'hào_quang', 'tổ_quốc', 'chj', 'khác', 'ở', 'chỗ', 'quên', 'trừ', 'đam_mê', 'đam', 'mỹ', 'lí', 'hôn', 'oxygen', 'not', 'included', 'đừng', 'bi', 'bản_quyền', 'nhóm', 'nhé', 'lão', 'nếu', 'bọn', 'giữ', 'thế', 'vô', 'minecraft', 'rìu', 'chém', 'giải', 'chả', 'lẻ', 'mai', 'khổ', 'lam', 'review', 'di', 'hôn_nhân', 'quyết_định', 'đấm', 'edit', 'vẽ', 'phần_mềm', 'giờ', 'bố', 'cx', 'gặp', 'fan', 'exe', 'cù', 'thơi', 'truyện', 'xãy', 'hello', 'tất_cả', 'đều', 'hôm', 'xin_lỗi', 'cố_gắng', 'nek', 'hèn_gì', 'vụ', 'mơ', 'ngoại_tình', 'ghen', 'chắc', 'đáng', 'sợ', 'xự', 'bình', 'an', 'bếp', 'lửa', 'ak', 'haha', 'cute', 'ấm', 'đá', 'khỏi', 'nhá', 'moazzz', 'cuốn', 'xử', 'trước', 'đệ', 'hiếu', 'pc', 'nè', 'sẽ', 'ai', 'hút', 'ô', 'mô', 'kappa', 'zing', 'speed', 'mobile', 'hãy', 'bốc_phét', 'lớp', 'tính_toán', 'kĩ', 'sinh', 'gay', 'ạnh', 'lô_a', 'thử', 'game', 'night', 'with', 'nó']\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjvoCKFAsKEK"
   },
   "source": [
    "# VI.Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8oYeHeBL-wqO"
   },
   "source": [
    "def dot_product(x, kernel):\n",
    "\tif K.backend() == 'tensorflow':\n",
    "\t\treturn K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "\telse:\n",
    "\t\treturn K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "\tdef __init__(self,\n",
    "\t\t\t\t W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "\t\t\t\t W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "\t\t\t\t bias=True, **kwargs):\n",
    "\n",
    "\t\tself.supports_masking = True\n",
    "\t\tself.init = initializers.get('glorot_uniform')\n",
    "\n",
    "\t\tself.W_regularizer = regularizers.get(W_regularizer)\n",
    "\t\tself.u_regularizer = regularizers.get(u_regularizer)\n",
    "\t\tself.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "\t\tself.W_constraint = constraints.get(W_constraint)\n",
    "\t\tself.u_constraint = constraints.get(u_constraint)\n",
    "\t\tself.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "\t\tself.bias = bias\n",
    "\t\tsuper(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "\tdef build(self, input_shape):\n",
    "\t\tassert len(input_shape) == 3\n",
    "\n",
    "\t\tself.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "\t\t\t\t\t\t\t\t initializer=self.init,\n",
    "\t\t\t\t\t\t\t\t name='{}_W'.format(self.name),\n",
    "\t\t\t\t\t\t\t\t regularizer=self.W_regularizer,\n",
    "\t\t\t\t\t\t\t\t constraint=self.W_constraint)\n",
    "\t\tif self.bias:\n",
    "\t\t\tself.b = self.add_weight((input_shape[-1],),\n",
    "\t\t\t\t\t\t\t\t\t initializer='zero',\n",
    "\t\t\t\t\t\t\t\t\t name='{}_b'.format(self.name),\n",
    "\t\t\t\t\t\t\t\t\t regularizer=self.b_regularizer,\n",
    "\t\t\t\t\t\t\t\t\t constraint=self.b_constraint)\n",
    "\n",
    "\t\tself.u = self.add_weight((input_shape[-1],),\n",
    "\t\t\t\t\t\t\t\t initializer=self.init,\n",
    "\t\t\t\t\t\t\t\t name='{}_u'.format(self.name),\n",
    "\t\t\t\t\t\t\t\t regularizer=self.u_regularizer,\n",
    "\t\t\t\t\t\t\t\t constraint=self.u_constraint)\n",
    "\n",
    "\t\tsuper(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "\tdef compute_mask(self, input, input_mask=None):\n",
    "\t\t# do not pass the mask to the next layers\n",
    "\t\treturn None\n",
    "\n",
    "\tdef call(self, x, mask=None):\n",
    "\t\tuit = dot_product(x, self.W)\n",
    "\n",
    "\t\tif self.bias:\n",
    "\t\t\tuit += self.b\n",
    "\n",
    "\t\tuit = K.tanh(uit)\n",
    "\t\tait = dot_product(uit, self.u)\n",
    "\n",
    "\t\ta = K.exp(ait)\n",
    "\n",
    "\t\tif mask is not None:\n",
    "\t\t\ta *= K.cast(mask, K.floatx())\n",
    "\t\ta /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "\t\ta = K.expand_dims(a)\n",
    "\t\tweighted_input = x * a\n",
    "\t\t\n",
    "\t\treturn weighted_input\n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\treturn input_shape[0], input_shape[1], input_shape[2]\n",
    "\t\n",
    "class Addition(Layer):\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(Addition, self).__init__(**kwargs)\n",
    "\n",
    "\tdef build(self, input_shape):\n",
    "\t\tself.output_dim = input_shape[-1]\n",
    "\t\tsuper(Addition, self).build(input_shape)\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\treturn K.sum(x, axis=1)\n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\treturn (input_shape[0], self.output_dim)"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrCQ522csUQ0"
   },
   "source": [
    "## 1.Build mode LSTM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8b-4SQdfzFQ8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0bb5d878-6828-4932-c2b6-642ef0a63b68"
   },
   "source": [
    "filter_nums = 256 # best 128\n",
    "def build_model():\n",
    "        inputs  = Input(shape=(maxLength, ), dtype='float64', name='inputs')    \n",
    "        embedding_layer = Embedding(input_vocab_size,EMBEDDING_DIM,weights=[embedding_matrix], input_length=maxLength, trainable=True,name = 'word_emb')(inputs)\n",
    "        embedding_layer = SpatialDropout1D(0.75)(embedding_layer)\n",
    "                \n",
    "              \n",
    "        lstm_feature1 = CuDNNLSTM(filter_nums, return_sequences=True)(embedding_layer)\n",
    "\n",
    "        att1 = AttentionWithContext()(lstm_feature1)\n",
    "        att1 = Addition()(att1)\n",
    "\n",
    "        fc1 = Dropout(0.5)(Dense(256, name = 'dense_1')(att1))\n",
    "        output1 = Dense(len(classes),name=\"output1\", activation='softmax')(fc1)\n",
    "\n",
    "    \n",
    "        # define optimizer\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=output1)\n",
    "        tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "        \n",
    "        model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        history = model.fit(X_train_encode, np.array(y_train_encode), validation_data = (X_val_encode,np.array(y_val_encode)) , batch_size=50, epochs=100,callbacks=[tensorBoardCallback])\n",
    "        return model\n",
    "\n",
    "model = build_model()"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 100 samples, validate on 100 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 1.8981 - acc: 0.3600 - val_loss: 1.9148 - val_acc: 0.1900\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 807us/step - loss: 1.7388 - acc: 0.5000 - val_loss: 2.2418 - val_acc: 0.1900\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 684us/step - loss: 1.5215 - acc: 0.5300 - val_loss: 3.1445 - val_acc: 0.1900\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 670us/step - loss: 1.4201 - acc: 0.5300 - val_loss: 3.3296 - val_acc: 0.1900\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 648us/step - loss: 1.4435 - acc: 0.5300 - val_loss: 2.7502 - val_acc: 0.1900\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 514us/step - loss: 1.2846 - acc: 0.5400 - val_loss: 2.2758 - val_acc: 0.2600\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 462us/step - loss: 1.2447 - acc: 0.6000 - val_loss: 2.0905 - val_acc: 0.2700\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 453us/step - loss: 1.2461 - acc: 0.5800 - val_loss: 2.0749 - val_acc: 0.2500\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 470us/step - loss: 1.1865 - acc: 0.6000 - val_loss: 2.1223 - val_acc: 0.2600\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 471us/step - loss: 1.1309 - acc: 0.6200 - val_loss: 2.2212 - val_acc: 0.2600\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 432us/step - loss: 1.0674 - acc: 0.6300 - val_loss: 2.3355 - val_acc: 0.2300\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 452us/step - loss: 1.1099 - acc: 0.6400 - val_loss: 2.3654 - val_acc: 0.2600\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 482us/step - loss: 1.0122 - acc: 0.6400 - val_loss: 2.2999 - val_acc: 0.2400\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 444us/step - loss: 1.0767 - acc: 0.6600 - val_loss: 2.2020 - val_acc: 0.2500\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 475us/step - loss: 0.9650 - acc: 0.6500 - val_loss: 2.0983 - val_acc: 0.2800\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 478us/step - loss: 0.8884 - acc: 0.6400 - val_loss: 2.0466 - val_acc: 0.2900\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 496us/step - loss: 0.9983 - acc: 0.6800 - val_loss: 2.0036 - val_acc: 0.3100\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 486us/step - loss: 0.8957 - acc: 0.6800 - val_loss: 2.0020 - val_acc: 0.2900\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 488us/step - loss: 0.8860 - acc: 0.7100 - val_loss: 2.0258 - val_acc: 0.3000\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 466us/step - loss: 0.9205 - acc: 0.7000 - val_loss: 2.0420 - val_acc: 0.3000\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 458us/step - loss: 0.8253 - acc: 0.7100 - val_loss: 2.1113 - val_acc: 0.2900\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 473us/step - loss: 0.8354 - acc: 0.6700 - val_loss: 2.2001 - val_acc: 0.2900\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 451us/step - loss: 0.8597 - acc: 0.7300 - val_loss: 2.2118 - val_acc: 0.3000\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 471us/step - loss: 0.7260 - acc: 0.7200 - val_loss: 2.2046 - val_acc: 0.3000\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 490us/step - loss: 0.7262 - acc: 0.7400 - val_loss: 2.2325 - val_acc: 0.2900\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 481us/step - loss: 0.6914 - acc: 0.7500 - val_loss: 2.1850 - val_acc: 0.2900\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 500us/step - loss: 0.7595 - acc: 0.7600 - val_loss: 2.1314 - val_acc: 0.3100\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 496us/step - loss: 0.6173 - acc: 0.7900 - val_loss: 2.1345 - val_acc: 0.3200\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 495us/step - loss: 0.6054 - acc: 0.7900 - val_loss: 2.1726 - val_acc: 0.3200\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 462us/step - loss: 0.6112 - acc: 0.7900 - val_loss: 2.2093 - val_acc: 0.3200\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 459us/step - loss: 0.5442 - acc: 0.8000 - val_loss: 2.2622 - val_acc: 0.3200\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 464us/step - loss: 0.4620 - acc: 0.8200 - val_loss: 2.3238 - val_acc: 0.3100\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 502us/step - loss: 0.5477 - acc: 0.8300 - val_loss: 2.3660 - val_acc: 0.2900\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 480us/step - loss: 0.4719 - acc: 0.8200 - val_loss: 2.4588 - val_acc: 0.2900\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 495us/step - loss: 0.4061 - acc: 0.8700 - val_loss: 2.5881 - val_acc: 0.3000\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 496us/step - loss: 0.5057 - acc: 0.8200 - val_loss: 2.7433 - val_acc: 0.3000\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 492us/step - loss: 0.5631 - acc: 0.8600 - val_loss: 2.8589 - val_acc: 0.3000\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 503us/step - loss: 0.4078 - acc: 0.8700 - val_loss: 3.0500 - val_acc: 0.2800\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 472us/step - loss: 0.3802 - acc: 0.9100 - val_loss: 2.6712 - val_acc: 0.2900\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 467us/step - loss: 0.3744 - acc: 0.8500 - val_loss: 2.6677 - val_acc: 0.3200\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 470us/step - loss: 0.3788 - acc: 0.8800 - val_loss: 2.7807 - val_acc: 0.3100\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 523us/step - loss: 0.3385 - acc: 0.8700 - val_loss: 2.9208 - val_acc: 0.3000\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 472us/step - loss: 0.3456 - acc: 0.8800 - val_loss: 3.0176 - val_acc: 0.2800\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 459us/step - loss: 0.3540 - acc: 0.8700 - val_loss: 3.1430 - val_acc: 0.2700\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 475us/step - loss: 0.3515 - acc: 0.8600 - val_loss: 3.3190 - val_acc: 0.2700\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 513us/step - loss: 0.3766 - acc: 0.8800 - val_loss: 3.5741 - val_acc: 0.2700\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 451us/step - loss: 0.2594 - acc: 0.9100 - val_loss: 3.6387 - val_acc: 0.2800\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 454us/step - loss: 0.2907 - acc: 0.9000 - val_loss: 3.6390 - val_acc: 0.2700\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 489us/step - loss: 0.2231 - acc: 0.9300 - val_loss: 3.6783 - val_acc: 0.2900\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 492us/step - loss: 0.2472 - acc: 0.9100 - val_loss: 3.7720 - val_acc: 0.2800\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 469us/step - loss: 0.3365 - acc: 0.8700 - val_loss: 3.7136 - val_acc: 0.2700\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 483us/step - loss: 0.2659 - acc: 0.9200 - val_loss: 3.5744 - val_acc: 0.3100\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 537us/step - loss: 0.2218 - acc: 0.9600 - val_loss: 3.4937 - val_acc: 0.3100\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 458us/step - loss: 0.2059 - acc: 0.9600 - val_loss: 3.4727 - val_acc: 0.3200\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 528us/step - loss: 0.2495 - acc: 0.9400 - val_loss: 3.4428 - val_acc: 0.3500\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 461us/step - loss: 0.1785 - acc: 0.9300 - val_loss: 3.5185 - val_acc: 0.3300\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 475us/step - loss: 0.1537 - acc: 0.9700 - val_loss: 3.6928 - val_acc: 0.3400\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 479us/step - loss: 0.2366 - acc: 0.9400 - val_loss: 3.9581 - val_acc: 0.3400\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 504us/step - loss: 0.1827 - acc: 0.9400 - val_loss: 4.3417 - val_acc: 0.2800\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 494us/step - loss: 0.2270 - acc: 0.9400 - val_loss: 4.3513 - val_acc: 0.2900\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 465us/step - loss: 0.2546 - acc: 0.9300 - val_loss: 4.0085 - val_acc: 0.3100\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 445us/step - loss: 0.1756 - acc: 0.9800 - val_loss: 3.9057 - val_acc: 0.3200\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 485us/step - loss: 0.2721 - acc: 0.9100 - val_loss: 3.9620 - val_acc: 0.3200\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 442us/step - loss: 0.1349 - acc: 0.9800 - val_loss: 4.1616 - val_acc: 0.3400\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 474us/step - loss: 0.1428 - acc: 0.9800 - val_loss: 4.3920 - val_acc: 0.3400\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 505us/step - loss: 0.1138 - acc: 0.9700 - val_loss: 4.1964 - val_acc: 0.3300\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 520us/step - loss: 0.1113 - acc: 0.9600 - val_loss: 4.2963 - val_acc: 0.3100\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 513us/step - loss: 0.2198 - acc: 0.9300 - val_loss: 4.4418 - val_acc: 0.3000\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 449us/step - loss: 0.1169 - acc: 0.9600 - val_loss: 4.5482 - val_acc: 0.2900\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 473us/step - loss: 0.1239 - acc: 0.9500 - val_loss: 4.6800 - val_acc: 0.2700\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 469us/step - loss: 0.1629 - acc: 0.9200 - val_loss: 4.9359 - val_acc: 0.2600\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 497us/step - loss: 0.0794 - acc: 0.9700 - val_loss: 5.1560 - val_acc: 0.2600\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 506us/step - loss: 0.1206 - acc: 0.9800 - val_loss: 5.3176 - val_acc: 0.2500\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 502us/step - loss: 0.1881 - acc: 0.9600 - val_loss: 5.3247 - val_acc: 0.2800\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 524us/step - loss: 0.1512 - acc: 0.9500 - val_loss: 5.3139 - val_acc: 0.3200\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 486us/step - loss: 0.1147 - acc: 0.9700 - val_loss: 5.2621 - val_acc: 0.3000\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 477us/step - loss: 0.1402 - acc: 0.9200 - val_loss: 5.1156 - val_acc: 0.2900\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 469us/step - loss: 0.0942 - acc: 0.9600 - val_loss: 4.9001 - val_acc: 0.2500\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 474us/step - loss: 0.0595 - acc: 1.0000 - val_loss: 4.8611 - val_acc: 0.2900\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 469us/step - loss: 0.0540 - acc: 0.9800 - val_loss: 4.9046 - val_acc: 0.2900\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 481us/step - loss: 0.2419 - acc: 0.9300 - val_loss: 5.1126 - val_acc: 0.2600\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 499us/step - loss: 0.1635 - acc: 0.9800 - val_loss: 5.5026 - val_acc: 0.2700\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 510us/step - loss: 0.1608 - acc: 0.9800 - val_loss: 5.7920 - val_acc: 0.2600\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 477us/step - loss: 0.1215 - acc: 0.9800 - val_loss: 5.8083 - val_acc: 0.2600\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 472us/step - loss: 0.1537 - acc: 0.9600 - val_loss: 5.8217 - val_acc: 0.2600\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 480us/step - loss: 0.1353 - acc: 0.9600 - val_loss: 5.7215 - val_acc: 0.2600\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 497us/step - loss: 0.2439 - acc: 0.9500 - val_loss: 5.7516 - val_acc: 0.2600\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 493us/step - loss: 0.1048 - acc: 0.9700 - val_loss: 5.7576 - val_acc: 0.2600\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 495us/step - loss: 0.1498 - acc: 0.9400 - val_loss: 5.5845 - val_acc: 0.2500\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 504us/step - loss: 0.0930 - acc: 0.9700 - val_loss: 5.3822 - val_acc: 0.2600\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 502us/step - loss: 0.0847 - acc: 0.9900 - val_loss: 5.2518 - val_acc: 0.2600\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 502us/step - loss: 0.1833 - acc: 0.9600 - val_loss: 5.2515 - val_acc: 0.2800\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 503us/step - loss: 0.1219 - acc: 0.9600 - val_loss: 5.2506 - val_acc: 0.2700\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 472us/step - loss: 0.0651 - acc: 0.9700 - val_loss: 5.4391 - val_acc: 0.2800\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 558us/step - loss: 0.1396 - acc: 0.9700 - val_loss: 5.7734 - val_acc: 0.2800\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 522us/step - loss: 0.1093 - acc: 0.9500 - val_loss: 6.1853 - val_acc: 0.2700\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 470us/step - loss: 0.1456 - acc: 0.9500 - val_loss: 6.2615 - val_acc: 0.2600\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 483us/step - loss: 0.1467 - acc: 0.9700 - val_loss: 5.8550 - val_acc: 0.2800\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 495us/step - loss: 0.0986 - acc: 0.9600 - val_loss: 5.4193 - val_acc: 0.2700\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 449us/step - loss: 0.0837 - acc: 0.9800 - val_loss: 5.1082 - val_acc: 0.2900\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MmCoU4ksp67"
   },
   "source": [
    "## 2.Predict the results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cQwJ75p_WdBJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f6fe4093-d17a-4e5b-ae86-dcd77bad9963"
   },
   "source": [
    "X_test_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_test), maxlen=maxLength,padding=\"post\"))\n",
    "test_length = len(X_test_encode)\n",
    "\n",
    "y_predict = []\n",
    "predicted = model.predict(X_test_encode)\n",
    "for predict in predicted:\n",
    "    index2, value = max(enumerate(predict), key=operator.itemgetter(1))\n",
    "    y_predict.append(classes[index2])\n",
    "    \n",
    "print(y_predict[0])"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Sadness\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "CWnvBNeI0joN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cb7f3387-3eee-46b1-921a-af6eb5e64ca7"
   },
   "source": [
    "print(y_predict)"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "['Sadness', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Other', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Sadness', 'Other', 'Other', 'Enjoyment', 'Sadness', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Sadness', 'Other', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Sadness', 'Enjoyment', 'Other', 'Sadness', 'Enjoyment', 'Enjoyment', 'Sadness', 'Enjoyment', 'Other', 'Sadness', 'Enjoyment', 'Other', 'Other', 'Enjoyment', 'Enjoyment', 'Sadness', 'Enjoyment', 'Sadness', 'Other', 'Other', 'Disgust', 'Enjoyment', 'Other', 'Other', 'Other', 'Enjoyment', 'Other', 'Enjoyment', 'Disgust', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Disgust', 'Enjoyment', 'Other', 'Other', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Other', 'Disgust', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Surprise', 'Enjoyment', 'Other', 'Disgust', 'Enjoyment', 'Other', 'Other', 'Sadness', 'Enjoyment', 'Sadness', 'Sadness', 'Enjoyment', 'Disgust', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Disgust']\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VXZOPGcs9O_"
   },
   "source": [
    "## 3.Report the performance metrics (Accuracy, F1-score...)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WoEymwgf0joR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "24679174-aafc-4f1a-ebd1-a497755f7247"
   },
   "source": [
    "precision = precision_score(y_test, y_predict, average='weighted')\n",
    "recall = recall_score(y_test, y_predict, average='weighted')\n",
    "f1score = f1_score(y_test, y_predict, average='micro')\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "print(\"Result model LSTM + Attention layer\")\n",
    "print(\"Results of the models\")\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-Score: \", f1score)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "print(classification_report(y_test,y_predict))"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Result model LSTM + Attention layer\n",
      "Results of the models\n",
      "Precision:  0.1440322580645161\n",
      "Recall:  0.18\n",
      "F1-Score:  0.18\n",
      "Accuracy:  0.18\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.00      0.00      0.00         7\n",
      "     Disgust       0.14      0.05      0.07        21\n",
      "   Enjoyment       0.25      0.35      0.29        34\n",
      "        Fear       0.00      0.00      0.00         1\n",
      "       Other       0.16      0.28      0.20        18\n",
      "     Sadness       0.00      0.00      0.00         3\n",
      "    Surprise       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.18       100\n",
      "   macro avg       0.08      0.10      0.08       100\n",
      "weighted avg       0.14      0.18      0.15       100\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2pll3z0tLDW"
   },
   "source": [
    "# VII.Enter the demo program into 1 sentence"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3WyndWU_0joU"
   },
   "source": [
    "def demo(str):\n",
    "  demo_pre = clean_doc(str)\n",
    "  X_demo_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences([demo_pre]), maxlen=maxLength,padding=\"post\"))\n",
    "  predicted = model.predict(X_demo_encode)\n",
    "  index2, value = max(enumerate(predicted[0]), key=operator.itemgetter(1))\n",
    "  print(str)\n",
    "  print(\"Predict the results:\", classes[index2])"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dVJw8Vi5thbF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a7afc8a4-41cf-42c5-e8e9-6f917bdfc263"
   },
   "source": [
    "demo('một giọng nói của người trải qua sự bi đát.mong được lên tv some day')"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "một giọng nói của người trải qua sự bi đát.mong được lên tv some day\n",
      "Predict the results: Enjoyment\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}